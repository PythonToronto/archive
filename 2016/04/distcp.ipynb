{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking the java out of pythonic big data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "<img src=\"http://www.logospike.com/wp-content/uploads/2014/11/Java_logo-2.jpg\" width=300> **Java** is ubiquitous in the world of Big Data, largely because of the prominence of Hadoop and tools built on it. There are distinct parts to java's place in this infrastructure (with a few example names people will recognize):\n",
    "\n",
    "- services: long-running java processes managing the system, that other user processes talk to.\n",
    "    - HDFS for resilient distributed data storage\n",
    "    - YARN for allocating resources\n",
    "- runtimes: user code using java execution engines to process data\n",
    "    - map-reduce\n",
    "- libraries: to enable user code to do what it needs\n",
    "    - to interact with the services and runtimes, above\n",
    "    - other functionality, like access to data formats (avro, parquet...)\n",
    "\n",
    "It is no surprise that top-level data engines like pig, hive, etc., are written in java.\n",
    "\n",
    "    \n",
    "### What about spark?\n",
    "\n",
    "Spark has risen extremely rapidly to be perhaps the most important big data tool around, or certainly the most talked-about platform.\n",
    "\n",
    "Along with R, python is one of the API languages to run computations in the spark framework. The execution engine itself if written in scala, running on Java Virtual Machines (JVMs). Although scala is growing quickly, massive amount of spark programming actually takes place in python (and R), because\n",
    "- many data-oriented coders are more comfortable in python\n",
    "- there is an excellent ecosystem of data science related libraries (e.g., sklearn)\n",
    "- usual benefits of rapid development with fewer lines of code and interactivity\n",
    "\n",
    "Whilst opening up spark to python is great for us, python will always lag \n",
    "Running a python operation on data via spark involves multiple serialization steps python<->java\n",
    "<img src=\"./spark_python.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidestepping java\n",
    "\n",
    "We can, of course, call java functions from within python or the shell, but they are very slow, having to start up a JVM each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwxrwt   - hdfs hadoop          0 2016-04-18 19:11 /tmp\n",
      "drwxr-xr-t   - hdfs hadoop          0 2016-04-18 19:11 /user\n",
      "CPU times: user 29.8 ms, sys: 15.6 ms, total: 45.4 ms\n",
      "Wall time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, there had been several attempts to make interface libraries to talk to big-data services in native python; see, e.g., [snakebite](https://github.com/spotify/snakebite), which uses RCP directly to talk to the HDFS namenode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate two small libraries to connect python to big data. They are useful in their own right, or in combination with dask-distributed, a pure python cluster execution engine.\n",
    "\n",
    "- [hdfs3](http://hdfs3.readthedocs.org), based on [libhdfs3](http://pivotalrd.github.io/libhdfs3/) (c++, from pivotal) for HDFS\n",
    "- [s3fs](http://s3fs.readthedocs.org), based on [boto3](https://boto3.readthedocs.org/) for Amazon S3\n",
    "\n",
    "```\n",
    "> conda install hdfs3\n",
    "> conda install s3fs\n",
    "```\n",
    "\n",
    "Both provide an almost identical pythonic API for filesystem manipulation, and access to files compatible with the standard python file object. The APIs of the two projects are purposely very similar.\n",
    "\n",
    "Here are a few examples of familiar file system operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distributed-test/csv/',\n",
       " 'distributed-test/gzip-json/',\n",
       " 'distributed-test/nested/',\n",
       " 'distributed-test/test/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdfs3, s3fs\n",
    "hdfs = hdfs3.HDFileSystem()\n",
    "s3 = s3fs.S3FileSystem()\n",
    "s3.ls('distributed-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same operation as the hadoop CLI `ls` above, a factor of ~450x faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 ms, sys: 96 Âµs, total: 1.29 ms\n",
      "Wall time: 4.73 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'block_size': 0,\n",
       "  'group': 'hadoop',\n",
       "  'kind': 'directory',\n",
       "  'last_access': 0,\n",
       "  'last_mod': 1461006682,\n",
       "  'name': '//tmp',\n",
       "  'owner': 'hdfs',\n",
       "  'permissions': 1023,\n",
       "  'replication': 0,\n",
       "  'size': 0},\n",
       " {'block_size': 0,\n",
       "  'group': 'hadoop',\n",
       "  'kind': 'directory',\n",
       "  'last_access': 0,\n",
       "  'last_mod': 1461006705,\n",
       "  'name': '//user',\n",
       "  'owner': 'hdfs',\n",
       "  'permissions': 1005,\n",
       "  'replication': 0,\n",
       "  'size': 0}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "hdfs.ls('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distributed-test/test/accounts.1.json': 133,\n",
       " 'distributed-test/test/accounts.2.json': 133}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.du('distributed-test/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distributed-test/csv/2014/',\n",
       " 'distributed-test/csv/2014/2014-01-01.csv',\n",
       " 'distributed-test/csv/2014/2014-01-02.csv',\n",
       " 'distributed-test/csv/2014/2014-01-03.csv',\n",
       " 'distributed-test/csv/2015/',\n",
       " 'distributed-test/csv/2015/2015-01-01.csv',\n",
       " 'distributed-test/csv/2015/2015-01-02.csv',\n",
       " 'distributed-test/csv/2015/2015-01-03.csv',\n",
       " 'distributed-test/csv/gzip/',\n",
       " 'distributed-test/csv/gzip/2015-01-01.csv.gz',\n",
       " 'distributed-test/csv/gzip/2015-01-02.csv.gz',\n",
       " 'distributed-test/csv/gzip/2015-01-03.csv.gz']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.walk('distributed-test/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distributed-test/csv/2014/2014-01-01.csv',\n",
       " 'distributed-test/csv/2014/2014-01-02.csv',\n",
       " 'distributed-test/csv/2014/2014-01-03.csv',\n",
       " 'distributed-test/csv/2015/2015-01-01.csv',\n",
       " 'distributed-test/csv/2015/2015-01-02.csv',\n",
       " 'distributed-test/csv/2015/2015-01-03.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.glob('distributed-test/csv/*/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'name,amount,id\\nAlice,100,1\\nBob,200,2\\nCharlie,300,3\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.cat('distributed-test/csv/2014/2014-01-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,amount,id\r\n",
      "Alice,100,1\r\n",
      "Bob,200,2\r\n",
      "Charlie,300,3\r\n"
     ]
    }
   ],
   "source": [
    "s3.get('distributed-test/csv/2014/2014-01-01.csv', 'myfile.csv')\n",
    "%cat myfile.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and repeating the simple `ls` with a new interpreter: 11x faster than hadoop CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'size': 0, 'owner': 'hdfs', 'permissions': 1023, 'replication': 0, 'kind': 'directory', 'last_mod': 1461006682, 'name': '//tmp', 'group': 'hadoop', 'last_access': 0, 'block_size': 0}, {'size': 0, 'owner': 'hdfs', 'permissions': 1005, 'replication': 0, 'kind': 'directory', 'last_mod': 1461006705, 'name': '//user', 'group': 'hadoop', 'last_access': 0, 'block_size': 0}]\n",
      "CPU times: user 0 ns, sys: 7.64 ms, total: 7.64 ms\n",
      "Wall time: 198 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!/opt/anaconda/bin/python -c \"import hdfs3; print(hdfs3.HDFileSystem().ls('/'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading only a specific block from a file, with delimiters (which could be multi-byte sentinel values). In this case, we get exactly one whole line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Alice,100,1\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# block-wise access with delimiters:\n",
    "s3.read_block('distributed-test/csv/2014/2014-01-01.csv', 5, 10, delimiter=b'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HDFS, we also have access to information about which data node holds which blocks of any given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hosts': [b'ip-172-31-2-252.ec2.internal',\n",
       "   b'ip-172-31-2-253.ec2.internal',\n",
       "   b'ip-172-31-2-254.ec2.internal'],\n",
       "  'length': 51,\n",
       "  'offset': 0}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs.put('myfile.csv', '/myfile.csv')\n",
    "hdfs.get_block_locations('/myfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Both libraries provide a python file object compliant interface with (binary) read, write and append modes; files are line-iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b't,id\\nAlice'\n",
      "\n",
      "b'name,amount,id'\n",
      "b'Alice,100,1'\n",
      "b'Bob,200,2'\n",
      "b'Charlie,300,3'\n",
      "b''\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'Hello World'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with hdfs.open('/myfile.csv', 'rb') as f:\n",
    "    f.seek(10)\n",
    "    print(f.read(10))\n",
    "\n",
    "print()\n",
    "    \n",
    "with hdfs.open('/myfile.csv', 'rb') as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "\n",
    "with hdfs.open('/temporary', 'wb') as f:\n",
    "    f.write(b'Hello World')    \n",
    "    \n",
    "hdfs.cat('/temporary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other libraries can use hdfs3 and s3fs files as if they were ordinary python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>amount</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  amount  id\n",
       "0    Alice     100   1\n",
       "1      Bob     200   2\n",
       "2  Charlie     300   3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compatibility\n",
    "import pandas as pd\n",
    "import gzip\n",
    "with s3.open('distributed-test/csv/gzip/2015-01-01.csv.gz', 'rb') as f:\n",
    "    df = pd.read_csv(gzip.open(f))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional goodies:\n",
    "- HDFS reads/writes are local (short-circuit) if possible. Since we can find which node has which block, we can ship the task to the data, rather than the other way around.\n",
    "- S3 files are cached with read-ahead in read mode\n",
    "- S3 files are uploaded in chunks and combined server-side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dask-distributed\n",
    "\n",
    "So we can access Big Data - now we want to process it on a cluster.\n",
    "\n",
    "Lets look at some example data: 26GB of CSV from the NYC taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dask-data/nyc-taxi/2014/yellow_tripdata_2014-01.csv': 2324817062,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-02.csv': 2205460986,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-03.csv': 2604044841,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-04.csv': 2463299452,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-05.csv': 2488952100,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-06.csv': 2287263479,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-07.csv': 2204602640,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-08.csv': 2133936569,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-09.csv': 2260069774,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-10.csv': 2409216587,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-11.csv': 2235168890,\n",
       "  'dask-data/nyc-taxi/2014/yellow_tripdata_2014-12.csv': 2198671160},\n",
       " 25.905206371098757)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.du('dask-data/nyc-taxi/2014/'), s3.du('dask-data/nyc-taxi/2014/', total=True) / 2**30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendor_id, pickup_datetime, dropoff_datetime, passenger_count, trip_distance, pickup_longitude, pickup_latitude, rate_code, store_and_fwd_flag, dropoff_longitude, dropoff_latitude, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount\n",
      "\r\n",
      "CMT,2014-01-09 20:45:25,2014-01-09 20:52:31,1,0.69999999999999996,-73.994770000000003,40.736828000000003,1,N,-73.982226999999995,40.731789999999997,CRD,6.5,0.5,0.5,1.3999999999999999,0,8.9000000000000004\r\n",
      "CMT,2014-01-09 20:46:12,2014-01-09 20:55:12,1,1.3999999999999999,-73.982392000000004,40.773381999999998,1,N,-73.960448999999997,40.763995000000001,CRD,8.5,0.5,0.5,1.8999999999999999,0,11.4\r\n",
      "CMT,2014-01-09 20:44:47,2014-01-09 20:59:46,2,2.2999999999999998,-73.988569999999996,40.739406000000002,1,N,-73.986626000000001,40.765217,CRD,11.5,0.5,0.5,1.5,0,14\r\n",
      "CMT,2014-01-09 20:44:57,2014-01-09 20:51:40,1,1.7,-73.960212999999996,40.770463999999997,1,N,-73.979862999999995,40.777050000000003,CRD,7.5,0.5,0.5,1.7,0,10.199999999999999\r\n",
      "CMT,2014-01-09 20:47:\n"
     ]
    }
   ],
   "source": [
    "# file structure\n",
    "print(s3.head('dask-data/nyc-taxi/2014/yellow_tripdata_2014-01.csv').decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dask-distributed scheduler and set of workers was already set up on the cluster. See [here](distributed.readthedocs.org/en/latest/) for a detailed description of the design and use of distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'172.31.2.252:36319': 2,\n",
       " '172.31.2.252:45257': 2,\n",
       " '172.31.2.253:33361': 2,\n",
       " '172.31.2.253:39447': 2,\n",
       " '172.31.2.254:45268': 2,\n",
       " '172.31.2.254:50737': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import distributed\n",
    "e = distributed.Executor('localhost:8786')\n",
    "e.ncores()\n",
    "# JSON interface on :9786\n",
    "# web interface on :8787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets transfer the data to the local HDFS. No parallelism is required for this, the first function could be run over the files serially. However, this demonstrated how `distcp` (which may be familiar from the map-reduce world) can be simply parallelised over the nodes using a single call to `map`. Since all the cores access the same HDFS, they can write locally, but the blocks are replicated for fault tolerance around the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download(origin, destination, host=hdfs.host, port=hdfs.port):\n",
    "    \"\"\" S3->HDFS streaming downloader \"\"\"\n",
    "    hdfs = hdfs3.HDFileSystem(host=host, port=port)\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    blocksize = 10**7\n",
    "    out = True\n",
    "    with s3.open(origin, 'rb') as f1, hdfs.open(destination, 'wb') as f2:\n",
    "        while out:\n",
    "            out = f1.read(blocksize)\n",
    "            f2.write(out)\n",
    "\n",
    "def distcp(inglob, outpath, executor):\n",
    "    \"\"\" Parallel distributed S3->HDFS downloader\"\"\"\n",
    "    hdfs = hdfs3.HDFileSystem()\n",
    "    hdfs.mkdir(outpath)\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    infiles = s3.glob(inglob)\n",
    "    outfiles = [outpath + f.rsplit('/', 1)[1] for f in infiles]\n",
    "    return executor.map(download, infiles, outfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the copy. Notice that `map`, above, returns *futures*, local handles to operations happening on the cluster, but that the client does not block. However, we can monitor its progress either here in the notebook, through the REST interface, or in the web UI. There is also a simplified text progressbar for console use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future: status: pending, key: download-67385a0f16fe62e8857908ec5795e303>\n"
     ]
    }
   ],
   "source": [
    "futures = distcp('dask-data/nyc-taxi/2014/*', '/NYC/', e)\n",
    "print(futures[0])\n",
    "distributed.progress(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.905206371098757"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs.du('/NYC', total=True)['/NYC'] / 2**30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask gives you multiple levels of entry, allowing direct low-level control or convenient abstractions:\n",
    "- direct access to the the dask DAG interface (e.g., for developers making new algorithms)\n",
    "- intermediate executor interface with map/submit/scatter etc., returning\n",
    "    - futures pointing to processes working on the cluster (eagerly evaluated remotely, but not blocking)\n",
    "    - imperative values for sets of tasks ready to be submitter (lazy evaluation)\n",
    "- collections\n",
    "    - `bag`: schema-less python objects, like dictionaries (e.g., from JSON) - manipulate with toolz\n",
    "    - `array`: numerical NDarrays with numpy functionality and linear algebra\n",
    "    - `dataframe`: tabular data with pandas functionality\n",
    "    \n",
    "As well as the distributed engine, dask is easily usable for thread- or process-based out-of-core processing on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher-level dask functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting global dask scheduler to use distributed\n"
     ]
    }
   ],
   "source": [
    "import distributed.hdfs\n",
    "# split set of files into many chunks, each of which becomes an in-memory \n",
    "# pandas dataframe, a pieces of the whole dataset\n",
    "df = distributed.hdfs.read_csv('/NYC/*', header='infer', skip_blank_lines=True,\n",
    "                                lazy=False, skipinitialspace=True)\n",
    "df = e.persist(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the cluster is working to load all of the data into memory but, again, the client is not blocked and we are free to do other work or sumit more tasks to the scheduler. Blocking only happens when we require a concrete result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = df.vendor_id.count()\n",
    "distributed.progress(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164883392"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in memory (with one small pandas data-frame for each HDFS block of the original data), querying the 165M-row data-set happens much faster, even when doing calculations on it on the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279682748\n",
      "CPU times: user 110 ms, sys: 0 ns, total: 110 ms\n",
      "Wall time: 955 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(df.passenger_count.sum().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444566140\n",
      "CPU times: user 160 ms, sys: 0 ns, total: 160 ms\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print((df.passenger_count + 1).sum().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 209 ms, sys: 5.2 ms, total: 214 ms\n",
      "Wall time: 1.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "passenger_count\n",
       "0      2.005099\n",
       "1      1.529201\n",
       "2      1.499112\n",
       "3      1.369163\n",
       "4      1.241804\n",
       "5      1.524903\n",
       "6      1.490462\n",
       "7      3.637299\n",
       "8      3.904592\n",
       "9      4.690079\n",
       "208    0.000000\n",
       "Name: tip_amount, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.groupby('passenger_count').tip_amount.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The wider python big-data ecosystem\n",
    "\n",
    "Many tools are being built to plug the gaps and introduce novel processign functionality - we are fleshing out the massive data clustered python ecosystem.\n",
    "\n",
    "- Xarray and upcoming projects based on dask\n",
    "- knit: YARN integration\n",
    "- dec2: quick and easy dask-distributed clusters\n",
    "- Anaconda for Cluster Management (free or as part of paid Anaconda)\n",
    "- avro format, parquet coming (others?)\n",
    "- Mosaic (data portal)/blaze server \n",
    "- ...\n",
    "\n",
    "<img src=\"http://www.h-online.com/security/imgs/46/4/5/3/9/3/9/NoMoreJava-be60b4a92a66c80f-be60b4a92a66c80f.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
